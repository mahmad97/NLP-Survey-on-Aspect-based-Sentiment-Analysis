Talk about adversarial training. Adding peturbations or noise to an input to fool the network to think that it is somehting entirely different but it actually is almost the same as the original so should be classified as the same. Adding a bit of noise and trying to get the same result as a human would be able to ignore that noise.
Apply this technique to word embedding space for text calssification.
Contribution: use the above adversarial training to fine-tune a BERT language model for aspect extraction and ASC. Outperforms general BERT as well as BERT-PT.
Adversarial training used for sentence classification before, not for ABSA.
AE: BERT word embedding layer used, each word vectorized and then classified as B(eginning), I(nside), O(utside) so aspects can be determined. ASC done by BERT model whose output then goes through a fully connected layer to produce +tive, -tive, neutral classification.
In preprocessing inputs with multiple aspects are repeated are repeated through SA as many times as the number of aspects. Each time the relevant ASC is done as a result.
(Use diagram) Do things normally until we get loss. Use the gradient of loss to create the adversarial examples.run the encoder on it again to get the adversarial loss. Use total loss. backpropogation is done through the sum of both the losses. want to minimize total loss as we want our model to classify correct as well as to be resistant to perturbations.
Describe embedding layer a bit (3 embeddings result)
Ran the code, getting results which are close enough to the reported ones
Hyperparameter changes leads to gains in one dataset (restaurant) while performance goes bad for the other dataset (laptop). This lack of consistency is attributed to the way the adversarial samples are being made where they negatively affect some inputs.
